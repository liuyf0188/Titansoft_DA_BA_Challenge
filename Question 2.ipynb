{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 codes and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the buyer data\n",
    "buyer_df = pd.read_csv('buyer.csv')\n",
    "\n",
    "# Convert the 'Annual income' column to numeric values, assuming 'k' stands for thousands\n",
    "buyer_df['Annual income'] = buyer_df['Annual income'].str.replace('k', '').astype(int) * 1000\n",
    "\n",
    "# Preprocess the data: convert categorical variables to numeric\n",
    "# Assuming the \"Gender\" column contains 'Male' and 'Female', and \"Married\" column contains 'TRUE' and 'FALSE'\n",
    "buyer_df['Gender'] = buyer_df['Gender'].map({'Male': 0, 'Female': 1})\n",
    "buyer_df['Married'] = buyer_df['Married'].astype(int)\n",
    "buyer_df['Buy'] = buyer_df['Buy'].astype(int)\n",
    "\n",
    "# Select features and target variable\n",
    "X = buyer_df[['Age', 'Gender', 'Annual income', 'Married']]\n",
    "y = buyer_df['Buy']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "[0.59431258]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Annual income    1.238016\n",
       "Age              0.518439\n",
       "Gender           0.334001\n",
       "Married          0.047491\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression pipeline\n",
    "logistic_pipeline = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\n",
    "\n",
    "# Fit the model on the full dataset (better for feature importance analysis)\n",
    "logistic_pipeline.fit(X, y)\n",
    "\n",
    "# Get the coefficients from the logistic regression model\n",
    "coefs = logistic_pipeline.named_steps['logisticregression'].coef_[0]\n",
    "features = X.columns\n",
    "\n",
    "# Now let's predict the probability of the new customer buying a house\n",
    "new_customer = pd.DataFrame({\n",
    "    'Age': [40],\n",
    "    'Gender': [1],  # Female\n",
    "    'Annual income': [310000],  # Corrected to full amount\n",
    "    'Married': [0]  # Unmarried\n",
    "})\n",
    "\n",
    "# Predict the probability of the new customer buying a house using the logistic regression model\n",
    "probability_of_buying_logistic = logistic_pipeline.predict_proba(new_customer)[:, 1]\n",
    "\n",
    "# Evaluate the logistic regression model on the train and test set\n",
    "logistic_pipeline.fit(X_train, y_train)\n",
    "logistic_predictions = logistic_pipeline.predict(X_test)\n",
    "logistic_report = classification_report(y_test, logistic_predictions, zero_division=0)\n",
    "\n",
    "print(logistic_report)\n",
    "print(probability_of_buying_logistic)\n",
    "\n",
    "# Map coefficients to features\n",
    "feature_importance = pd.Series(coefs, index=features).sort_values(key=abs, ascending=False)\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Prediction\n",
    "\n",
    "### Probability of New Customer Buying\n",
    "- The Logistic Regression model predicts a **59.4% probability** of the new customer purchasing a house.\n",
    "\n",
    "### Classification Report\n",
    "- The test set evaluation yields perfect scores:\n",
    "  - **Precision:** 100% for both classes.\n",
    "  - **Recall:** 100% for both classes.\n",
    "  - **F1-Score:** 100% for both classes.\n",
    "  - **Accuracy:** 100% overall.\n",
    "\n",
    "### Decision on Customer Acquisition\n",
    "- Considering the **59.4% probability** of purchase and the profile's similarity to previous buyers, if the expected revenue from the customer is greater than the high acquisition cost, acquiring the new customer could be a beneficial move.\n",
    "- The decision must also take into account strategic business objectives and the capacity to onboard new customers without affecting service quality for existing ones.\n",
    "\n",
    "### Notes on Evaluation\n",
    "- While the classification report indicates perfect metrics, the very small test set size means these scores do not necessarily indicate the model's performance in a real-world scenario.\n",
    "- More data and a larger test set would be needed to accurately gauge the model's effectiveness and reliability.\n",
    "\n",
    "## Logistic Regression Analysis for Customer Acquisition\n",
    "\n",
    "### Feature Importance from Logistic Regression\n",
    "- **Annual income** is the most significant predictor of whether a customer will buy a house.\n",
    "- **Age** also positively impacts the likelihood of a customer purchasing a house.\n",
    "- **Gender** has a positive, albeit smaller, effect on the prediction.\n",
    "- **Married** status shows minimal importance in the logistic regression model.\n",
    "\n",
    "### Model Selection Rationale\n",
    "- Logistic Regression was chosen for its simplicity and interpretability compared to more complex models like RandomForest.\n",
    "- It outputs probabilities directly through the sigmoid function, making it suitable for binary classification problems.\n",
    "- The model coefficients provide clear insights into the impact of each feature on the likelihood of a customer buying a house.\n",
    "\n",
    "### Limitations\n",
    "- Logistic Regression assumes a linear relationship between the log-odds of the dependent variable and each independent variable, which may not capture more complex patterns.\n",
    "- Feature importance is based on the scale of the data, so proper preprocessing steps such as standardization are crucial.\n",
    "- It may not perform as well as more complex models when the true relationship is not linear or if there are significant interactions between features.\n",
    "\n",
    "### Conclusion\n",
    "- The model suggests that the **annual income** is the most crucial factor in predicting house purchasing, followed by the customer's **age**.\n",
    "- Given the coefficients, we can interpret that as the **annual income** and **age** increase, so does the likelihood of a customer purchasing a house.\n",
    "- Since **married** status has a near-zero coefficient, it does not seem to be a decisive factor for purchasing within the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
